# Transformer

A PyTorch implementation of a Transformer model from scratch.

---

## Project Overview

This repository contains a from-scratch implementation of the Transformer architecture, inspired by the original paper "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)" by Vaswani et al. The code demonstrates the core components including multi-head attention, positional encoding, and the encoder-decoder structure.

---

## Features

- Custom implementation of multi-head self-attention
- Positional encoding added to input embeddings
- Transformer encoder and decoder layers
- Training and evaluation scripts
- Modular and easy-to-extend code structure

---

## Getting Started

### Prerequisites

- Python 3.8+
- PyTorch

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/dhavalantala/transformer.git
   cd transformer
